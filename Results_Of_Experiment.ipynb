{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a5886e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a5886e6",
        "outputId": "8fd694b8-8e1e-4c71-a952-f285f41bb9a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace 'cifake-dataset.zip' with your actual file name\n",
        "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('cifake_dataset')\n",
        "\n",
        "# List the contents to verify\n",
        "os.listdir('cifake_dataset')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "\n",
        "# === CONFIG ===\n",
        "base_dir = '/content/cifake_dataset'\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Paths\n",
        "train_fake_dir = os.path.join(base_dir, 'train', 'FAKE')\n",
        "train_real_dir = os.path.join(base_dir, 'train', 'REAL')\n",
        "\n",
        "# Get all image paths\n",
        "fake_images = [os.path.join(train_fake_dir, fname) for fname in os.listdir(train_fake_dir)][:600]\n",
        "real_images = [os.path.join(train_real_dir, fname) for fname in os.listdir(train_real_dir)][:600]\n",
        "\n",
        "# Combine paths and labels\n",
        "all_images = fake_images + real_images\n",
        "labels = [1] * 600 + [0] * 600  # 1 = FAKE, 0 = REAL\n",
        "\n",
        "# Shuffle\n",
        "combined = list(zip(all_images, labels))\n",
        "random.shuffle(combined)\n",
        "all_images, labels = zip(*combined)\n",
        "\n",
        "# === DATA AUGMENTATION FUNCTION ===\n",
        "def load_and_augment(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "    image = image / 255.0  # normalize\n",
        "    return image, label\n",
        "\n",
        "# === BUILD DATASET ===\n",
        "path_ds = tf.data.Dataset.from_tensor_slices((list(all_images), list(labels)))\n",
        "dataset = path_ds.map(load_and_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.shuffle(1200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "6PRkNs2K8kVh"
      },
      "id": "6PRkNs2K8kVh",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_images = fake_images[:600]\n",
        "real_images = real_images[:600]\n",
        "labels = [1]*600 + [0]*600\n"
      ],
      "metadata": {
        "id": "xxsCWVlk86xi"
      },
      "id": "xxsCWVlk86xi",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    all_images, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "EfbsXssH87f3"
      },
      "id": "EfbsXssH87f3",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Wrap training data\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(load_and_preprocess).shuffle(80).batch(16)\n",
        "\n",
        "# Wrap testing data\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "test_ds = test_ds.map(load_and_preprocess).batch(16)"
      ],
      "metadata": {
        "id": "lt5VeUh79G_I"
      },
      "id": "lt5VeUh79G_I",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# === LOAD FILE PATHS ===\n",
        "base_dir = 'cifake_dataset/train'  # <-- adjust if your folder is different\n",
        "fake_images = [os.path.join(base_dir, 'FAKE', fname) for fname in os.listdir(os.path.join(base_dir, 'FAKE'))]\n",
        "real_images = [os.path.join(base_dir, 'REAL', fname) for fname in os.listdir(os.path.join(base_dir, 'REAL'))]\n",
        "\n",
        "# === LIMIT TO 12,000 IMAGES (6000 each) ===\n",
        "fake_images = fake_images[:600]\n",
        "real_images = real_images[:600]\n",
        "all_images = fake_images + real_images\n",
        "labels = [0]*600 + [1]*600  # 0 = fake, 1 = real\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    all_images, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# === PREPROCESSING FUNCTION ===\n",
        "def load_and_preprocess(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "# === DATASET PIPELINE ===\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(load_and_preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "test_ds = test_ds.map(load_and_preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# === CNN MODEL ===\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === EARLY STOPPING ===\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# === TRAIN THE MODEL ===\n",
        "history = cnn_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUnrNCmeBNJb",
        "outputId": "ea96c49d-dfa4-4021-b3c8-63b6bab16076"
      },
      "id": "GUnrNCmeBNJb",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 4s/step - accuracy: 0.4968 - loss: 0.9409 - val_accuracy: 0.6333 - val_loss: 0.6539\n",
            "Epoch 2/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 5s/step - accuracy: 0.6138 - loss: 0.6523 - val_accuracy: 0.7333 - val_loss: 0.5670\n",
            "Epoch 3/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 4s/step - accuracy: 0.6814 - loss: 0.5784 - val_accuracy: 0.7125 - val_loss: 0.5535\n",
            "Epoch 4/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4s/step - accuracy: 0.7277 - loss: 0.5293 - val_accuracy: 0.7542 - val_loss: 0.5404\n",
            "Epoch 5/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 4s/step - accuracy: 0.7266 - loss: 0.5299 - val_accuracy: 0.7292 - val_loss: 0.5406\n",
            "Epoch 6/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 4s/step - accuracy: 0.7814 - loss: 0.4524 - val_accuracy: 0.7333 - val_loss: 0.5200\n",
            "Epoch 7/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 4s/step - accuracy: 0.7968 - loss: 0.4223 - val_accuracy: 0.7125 - val_loss: 0.5919\n",
            "Epoch 8/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 4s/step - accuracy: 0.8147 - loss: 0.4250 - val_accuracy: 0.7250 - val_loss: 0.5370\n",
            "Epoch 9/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 4s/step - accuracy: 0.8448 - loss: 0.3623 - val_accuracy: 0.7208 - val_loss: 0.5686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Pmg6uMjZWS",
        "outputId": "0f60d9ef-aab3-4228-d60c-2b53e1a8bde5"
      },
      "id": "B8Pmg6uMjZWS",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# === LOAD FILE PATHS ===\n",
        "base_dir = 'cifake_dataset/train'\n",
        "fake_images = [os.path.join(base_dir, 'FAKE', fname) for fname in os.listdir(os.path.join(base_dir, 'FAKE'))]\n",
        "real_images = [os.path.join(base_dir, 'REAL', fname) for fname in os.listdir(os.path.join(base_dir, 'REAL'))]\n",
        "\n",
        "# === LIMIT TO 1200 IMAGES (600 each) ===\n",
        "fake_images = fake_images[:600]\n",
        "real_images = real_images[:600]\n",
        "all_images = fake_images + real_images\n",
        "labels = [0]*600 + [1]*600  # 0 = fake, 1 = real\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    all_images, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# === VGG Preprocessing ===\n",
        "def load_and_preprocess(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "# === DATASET PIPELINE ===\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(load_and_preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "test_ds = test_ds.map(load_and_preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# === LOAD BASE VGG16 ===\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # freeze VGG layers\n",
        "\n",
        "# === BUILD MODEL ===\n",
        "vgg_model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "vgg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === EARLY STOPPING ===\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# === TRAIN THE MODEL ===\n",
        "history = vgg_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HxqRmUgk09r",
        "outputId": "863c7d75-6a47-4249-b6cb-955fa98f919f"
      },
      "id": "-HxqRmUgk09r",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 24s/step - accuracy: 0.6911 - loss: 6.0148 - val_accuracy: 0.8083 - val_loss: 3.4520\n",
            "Epoch 2/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 24s/step - accuracy: 0.8961 - loss: 1.5443 - val_accuracy: 0.8625 - val_loss: 1.3819\n",
            "Epoch 3/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 24s/step - accuracy: 0.9415 - loss: 0.4088 - val_accuracy: 0.8417 - val_loss: 1.3253\n",
            "Epoch 4/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 24s/step - accuracy: 0.9747 - loss: 0.1039 - val_accuracy: 0.8750 - val_loss: 1.0850\n",
            "Epoch 5/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 26s/step - accuracy: 0.9787 - loss: 0.0799 - val_accuracy: 0.8708 - val_loss: 0.6611\n",
            "Epoch 6/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 24s/step - accuracy: 0.9865 - loss: 0.0479 - val_accuracy: 0.8667 - val_loss: 0.6673\n",
            "Epoch 7/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 24s/step - accuracy: 0.9892 - loss: 0.0387 - val_accuracy: 0.8875 - val_loss: 0.6816\n",
            "Epoch 8/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 24s/step - accuracy: 0.9989 - loss: 0.0068 - val_accuracy: 0.8833 - val_loss: 0.6639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import ViTFeatureExtractor\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "IMAGE_SIZE = (224, 224)\n",
        "DATA_PATH = 'cifake_dataset/train'\n",
        "NUM_IMAGES_PER_CLASS = 600\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# === LOAD IMAGE PATHS ===\n",
        "fake_paths = [os.path.join(DATA_PATH, 'FAKE', fname) for fname in os.listdir(os.path.join(DATA_PATH, 'FAKE'))][:NUM_IMAGES_PER_CLASS]\n",
        "real_paths = [os.path.join(DATA_PATH, 'REAL', fname) for fname in os.listdir(os.path.join(DATA_PATH, 'REAL'))][:NUM_IMAGES_PER_CLASS]\n",
        "\n",
        "all_paths = fake_paths + real_paths\n",
        "all_labels = [0]*NUM_IMAGES_PER_CLASS + [1]*NUM_IMAGES_PER_CLASS  # 0 = FAKE, 1 = REAL\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    all_paths, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
        ")\n",
        "\n",
        "# === LOAD ViT FEATURE EXTRACTOR ===\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# === FEATURE EXTRACTION ===\n",
        "def extract_vit_features(image_paths, labels):\n",
        "    features = []\n",
        "    for path in image_paths:\n",
        "        img = Image.open(path).convert(\"RGB\").resize(IMAGE_SIZE)\n",
        "        inputs = feature_extractor(images=img, return_tensors=\"np\")\n",
        "        features.append(inputs[\"pixel_values\"][0])  # shape: (3, 224, 224)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "X_train, y_train = extract_vit_features(train_paths, train_labels)\n",
        "X_test, y_test = extract_vit_features(test_paths, test_labels)\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Labels: {y_train.shape}\")\n",
        "print(f\"Test shape:  {X_test.shape}, Labels: {y_test.shape}\")\n",
        "\n",
        "# === CLASSIFIER MODEL ===\n",
        "vit_classifier = Sequential([\n",
        "    Flatten(input_shape=(3, 224, 224)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "vit_classifier.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# === EARLY STOPPING CALLBACK ===\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# === TRAIN ===\n",
        "vit_classifier.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKbcVdIOrIED",
        "outputId": "731cabe6-dfb9-4faa-ef1b-c6e805e781e4"
      },
      "id": "EKbcVdIOrIED",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (960, 3, 224, 224), Labels: (960,)\n",
            "Test shape:  (240, 3, 224, 224), Labels: (240,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 644ms/step - accuracy: 0.5656 - loss: 14.7816 - val_accuracy: 0.6375 - val_loss: 14.3590\n",
            "Epoch 2/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 400ms/step - accuracy: 0.6408 - loss: 13.4218 - val_accuracy: 0.6708 - val_loss: 14.9627\n",
            "Epoch 3/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 403ms/step - accuracy: 0.7314 - loss: 8.4270 - val_accuracy: 0.6292 - val_loss: 11.1492\n",
            "Epoch 4/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 395ms/step - accuracy: 0.7205 - loss: 7.3934 - val_accuracy: 0.6833 - val_loss: 12.0369\n",
            "Epoch 5/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 399ms/step - accuracy: 0.7351 - loss: 5.4699 - val_accuracy: 0.6542 - val_loss: 11.9282\n",
            "Epoch 6/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 396ms/step - accuracy: 0.7321 - loss: 4.5511 - val_accuracy: 0.6792 - val_loss: 12.7364\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fb213555310>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# === Predict CNN ===\n",
        "y_pred_cnn = cnn_model.predict(test_ds)\n",
        "y_pred_cnn = (y_pred_cnn > 0.5).astype(int).flatten()\n",
        "\n",
        "# === True Labels ===\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "\n",
        "# === Scores ===\n",
        "print(\"===== CNN Test Results =====\")\n",
        "print(f\"Accuracy : {accuracy_score(y_true, y_pred_cnn):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_true, y_pred_cnn):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_true, y_pred_cnn):.4f}\")\n",
        "print(f\"F1 Score : {f1_score(y_true, y_pred_cnn):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6R62_M05aCv",
        "outputId": "ec079b8b-bdd4-4dcb-cf22-177229dc3633"
      },
      "id": "_6R62_M05aCv",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 835ms/step\n",
            "===== CNN Test Results =====\n",
            "Accuracy : 0.7333\n",
            "Precision: 0.7258\n",
            "Recall   : 0.7500\n",
            "F1 Score : 0.7377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict and threshold\n",
        "y_pred_vit = (vit_classifier.predict(X_test) > 0.5).astype(int).flatten()\n",
        "\n",
        "# Evaluation\n",
        "print(\"===== ViT Test Results =====\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test, y_pred_vit):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_vit):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test, y_pred_vit):.4f}\")\n",
        "print(f\"F1 Score : {f1_score(y_test, y_pred_vit):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO97wAoe6J-d",
        "outputId": "9dc2e620-767c-4874-f912-cd3e2e375cbb"
      },
      "id": "kO97wAoe6J-d",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fb2137aca40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "===== ViT Test Results =====\n",
            "Accuracy : 0.6292\n",
            "Precision: 0.6303\n",
            "Recall   : 0.6250\n",
            "F1 Score : 0.6276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Predict VGG ===\n",
        "y_pred_vgg = vgg_model.predict(test_ds)\n",
        "y_pred_vgg = (y_pred_vgg > 0.5).astype(int).flatten()\n",
        "\n",
        "# === Scores ===\n",
        "print(\"===== VGG Test Results =====\")\n",
        "print(f\"Accuracy : {accuracy_score(y_true, y_pred_vgg):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_true, y_pred_vgg):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_true, y_pred_vgg):.4f}\")\n",
        "print(f\"F1 Score : {f1_score(y_true, y_pred_vgg):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCj92TFz6zT6",
        "outputId": "11473ab1-6345-4703-93d2-1f5731f1edc9"
      },
      "id": "sCj92TFz6zT6",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 18s/step\n",
            "===== VGG Test Results =====\n",
            "Accuracy : 0.8708\n",
            "Precision: 0.8803\n",
            "Recall   : 0.8583\n",
            "F1 Score : 0.8692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model.save(\"vgg16_best_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJCAFRkm7Fzk",
        "outputId": "c855bd76-11da-4b28-c759-0582c746ee83"
      },
      "id": "SJCAFRkm7Fzk",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "vgg_model = load_model(\"vgg16_best_model.h5\")\n"
      ],
      "metadata": {
        "id": "6Xat9ub9a59I",
        "outputId": "09c118b2-6f0b-4120-ee3b-25dc65845b14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6Xat9ub9a59I",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio\n"
      ],
      "metadata": {
        "id": "ewJMaFP2bEr9",
        "outputId": "62922553-526f-4814-903a-a6163fc1d85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ewJMaFP2bEr9",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load your trained model\n",
        "model = load_model(\"vgg16_best_model.h5\")\n",
        "\n",
        "# Define prediction function\n",
        "def classify_image(img):\n",
        "    img = img.resize((224, 224))  # Resize to VGG16 input size\n",
        "    img_array = np.array(img)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    prediction = model.predict(img_array)[0][0]\n",
        "    label = \"FAKE\" if prediction < 0.5 else \"REAL\"\n",
        "    confidence = 1 - prediction if prediction < 0.5 else prediction\n",
        "    return f\"{label} ({confidence:.2%} confidence)\"\n",
        "\n",
        "# Launch Gradio interface\n",
        "gr.Interface(\n",
        "    fn=classify_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"VGG16 Image Classifier: Real or AI Generated?\",\n",
        "    description=\"Upload an image and the model will tell you if it's REAL or FAKE (AI-generated)\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "3pmA3hAdbF2z",
        "outputId": "9eef795a-8fc9-4321-8b3e-47c22b58e35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "id": "3pmA3hAdbF2z",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://46e25ff50291b90981.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://46e25ff50291b90981.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4LFZuQ9bL8K"
      },
      "id": "b4LFZuQ9bL8K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}